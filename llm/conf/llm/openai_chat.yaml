llm: 
  _target_    : llm.llm.OpenAIChat
  _partial_   : True

verbose       : True    

# The system message helps set the behavior of the assistant. 
# "You are a helpful assistant." or "You're an expert in deep learning."
system_message       : 'You are a helpful assistant.'

generation_params:
  # gpt-3.5-turbo
  model        :  gpt-3.5-turbo-0125 #gpt-3.5-turbo-instruct  #gpt-4-turbo-preview
  
  # The messages between the user and the bot
  #messages        : '' 

  # The maximum number of tokens to generate in the completion.
  max_tokens    : 300

  # Sampling temperature between 0 and 2. Higher values will make the output more random, 
  # while lower values like 0.2 will make it more focused and deterministic.
  temperature   : 0

  # An alternative to temperature, nucleus sampling. The model considers the results
  # of the toklen with top_p probability mass. So 0.1 means only the tokens comprising
  # the top 10% probability mass are considered.
  top_p         : 1

  # Whether to stream back partial progress
  stream        : False

  # up to 4 sequences that stop the generation
  stop          : '' 

  # Dictionary that can modify the likelihood of specified tokens appearing in the completion.
  # logit_bias: {}

  # Other params 
  frequency_penalty : 0
  presence_penalty  : 0  
  request_timeout   : 20
