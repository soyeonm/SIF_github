llm: 
  _target_    : llm.llm.OpenAI
  _partial_   : True

verbose       : True    

generation_params:
  # text-davinci-003, text-curie-001, text-babbage-001, text-ada-001
  #engine        : gpt-3.5-turbo-instruct #text-davinci-003
  model          : gpt-3.5-turbo-instruct


  # The prompt to start the generation from.
  prompt        : '' 

  # The maximum number of tokens to generate in the completion.
  max_tokens    : 300 #250 #100

  # Sampling temperature between 0 and 2. Higher values will make the output more random, 
  # while lower values like 0.2 will make it more focused and deterministic.
  temperature   : 0

  # An alternative to temperature, nucleus sampling. The model considers the results
  # of the toklen with top_p probability mass. So 0.1 means only the tokens comprising
  # the top 10% probability mass are considered.
  top_p         : 1

  # Returns the best `n` out of `best_of` completions made on server side
  n             : 1
  best_of       : 3

  # Whether to stream back partial progress
  stream        : False

  # Include log-probabilities of the top `logprobs` tokens.
  logprobs      : 0

  # up to 4 sequences that stop the generation
  stop          : '' 

  # Dictionary that can modify the likelihood of specified tokens appearing in the completion.
  # logit_bias: {}

  # Other params 
  frequency_penalty : 0
  presence_penalty  : 0  
  request_timeout   : 20
